from flask import Flask, request, jsonify, render_template
from flask_cors import CORS
import pdfplumber
import os
from transformers import pipeline, AutoTokenizer, AutoModelForQuestionAnswering
import torch
import re
import logging
from datetime import datetime

app = Flask(__name__)
CORS(app)

# Configura√ß√£o de logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Vari√°veis globais
PDF_PATH = 'Roteiro-de-Dsispensacao-Hanseniase-F.docx.pdf'
pdf_text = ""
qa_pipeline = None
tokenizer = None
model = None

def extract_pdf_text(pdf_path):
    """Extrai texto do PDF"""
    global pdf_text
    try:
        text = ""
        with pdfplumber.open(pdf_path) as pdf:
            for page_num, page in enumerate(pdf.pages):
                page_text = page.extract_text()
                if page_text:
                    text += f"\n--- P√°gina {page_num + 1} ---\n{page_text}\n"
        logger.info(f"PDF extra√≠do com sucesso. Total de caracteres: {len(text)}")
        return text
    except Exception as e:
        logger.error(f"Erro ao extrair PDF: {e}")
        return ""

def load_ai_model():
    """Carrega o modelo de IA gratuito do Hugging Face"""
    global qa_pipeline, tokenizer, model
    try:
        # Usando modelo gratuito e leve
        model_name = "deepset/roberta-base-squad2"
        logger.info(f"Carregando modelo: {model_name}")
        
        qa_pipeline = pipeline(
            "question-answering",
            model=model_name,
            tokenizer=model_name,
            device=-1 if not torch.cuda.is_available() else 0
        )
        logger.info("Modelo carregado com sucesso")
    except Exception as e:
        logger.error(f"Erro ao carregar modelo: {e}")
        qa_pipeline = None

def find_relevant_context(question, full_text, max_length=4000):
    """Encontra o contexto mais relevante para a pergunta"""
    # Divide o texto em chunks menores
    chunks = []
    chunk_size = 2000
    overlap = 200
    
    for i in range(0, len(full_text), chunk_size - overlap):
        chunk = full_text[i:i + chunk_size]
        if chunk.strip():
            chunks.append(chunk)
    
    # Se temos poucos chunks, retorna o texto completo
    if len(chunks) <= 2:
        return full_text[:max_length]
    
    # Para perguntas simples, usa o primeiro chunk
    if len(question.split()) <= 5:
        return chunks[0][:max_length]
    
    # Busca por palavras-chave na pergunta
    question_words = set(re.findall(r'\w+', question.lower()))
    
    best_chunk = chunks[0]
    best_score = 0
    
    for chunk in chunks:
        chunk_words = set(re.findall(r'\w+', chunk.lower()))
        common_words = question_words.intersection(chunk_words)
        score = len(common_words) / len(question_words) if question_words else 0
        
        if score > best_score:
            best_score = score
            best_chunk = chunk
    
    return best_chunk[:max_length]

def answer_question(question, persona):
    """Responde √† pergunta usando o modelo de IA"""
    global qa_pipeline, pdf_text
    
    if not qa_pipeline or not pdf_text:
        return fallback_response(persona, "Modelo n√£o dispon√≠vel")
    
    try:
        # Encontra contexto relevante
        context = find_relevant_context(question, pdf_text)
        
        # Faz a pergunta ao modelo
        result = qa_pipeline(
            question=question,
            context=context,
            max_answer_len=200,
            handle_impossible_answer=True
        )
        
        answer = result['answer'].strip()
        confidence = result['score']
        
        # Se a confian√ßa for baixa ou resposta vazia, usa fallback
        if not answer or confidence < 0.3:
            return fallback_response(persona, "Informa√ß√£o n√£o encontrada na tese")
        
        return format_persona_answer(answer, persona, confidence)
        
    except Exception as e:
        logger.error(f"Erro ao processar pergunta: {e}")
        return fallback_response(persona, "Erro t√©cnico")

def format_persona_answer(answer, persona, confidence):
    """Formata a resposta de acordo com a personalidade"""
    if persona == "dr_gasnelio":
        return {
            "answer": f"Dr. Gasnelio responde:\n\n{answer}\n\n*Baseado na tese sobre roteiro de dispensa√ß√£o para hansen√≠ase. Confian√ßa: {confidence:.1%}*",
            "persona": "dr_gasnelio",
            "confidence": confidence
        }
    elif persona == "ga":
        # Simplifica a resposta para o G√°
        simple_answer = simplify_text(answer)
        return {
            "answer": f"G√° explica:\n\n{simple_answer}\n\n*T√° na tese, pode confiar! üòä*",
            "persona": "ga",
            "confidence": confidence
        }
    else:
        return {
            "answer": answer,
            "persona": "default",
            "confidence": confidence
        }

def simplify_text(text):
    """Simplifica o texto para o G√°"""
    # Remove termos muito t√©cnicos e substitui por vers√µes mais simples
    replacements = {
        "dispensa√ß√£o": "entrega de rem√©dios",
        "farmac√™utico": "farmac√™utico",
        "medicamentos": "rem√©dios",
        "tratamento": "tratamento",
        "hansen√≠ase": "hansen√≠ase",
        "protocolo": "guia",
        "orienta√ß√£o": "explica√ß√£o",
        "paciente": "pessoa",
        "ades√£o": "seguir o tratamento",
        "posologia": "como tomar",
        "posologia": "como usar",
        "administra√ß√£o": "como tomar",
        "via de administra√ß√£o": "como tomar",
        "rea√ß√£o adversa": "efeito colateral",
        "intera√ß√£o medicamentosa": "mistura de rem√©dios"
    }
    
    simplified = text
    for technical, simple in replacements.items():
        simplified = simplified.replace(technical, simple)
    
    return simplified

def fallback_response(persona, reason=""):
    """Resposta de fallback quando n√£o encontra informa√ß√£o"""
    if persona == "dr_gasnelio":
        return {
            "answer": f"Dr. Gasnelio responde:\n\nDesculpe, n√£o encontrei essa informa√ß√£o espec√≠fica na minha tese sobre roteiro de dispensa√ß√£o para hansen√≠ase. {reason}\n\nPosso ajud√°-lo com outras quest√µes relacionadas ao tema da pesquisa.",
            "persona": "dr_gasnelio",
            "confidence": 0.0
        }
    elif persona == "ga":
        return {
            "answer": f"G√° responde:\n\nIh, essa eu n√£o sei! üòÖ {reason}\n\nS√≥ posso explicar coisas que est√£o na tese sobre hansen√≠ase e dispensa√ß√£o de rem√©dios. Pergunta outra coisa sobre o tema?",
            "persona": "ga",
            "confidence": 0.0
        }
    else:
        return {
            "answer": f"Desculpe, n√£o encontrei essa informa√ß√£o na tese. {reason}",
            "persona": "default",
            "confidence": 0.0
        }

@app.route('/')
def index():
    """P√°gina principal"""
    return render_template('index.html')

@app.route('/api/chat', methods=['POST'])
def chat_api():
    try:
        # Garante que o corpo √© JSON v√°lido
        if not request.is_json:
            return jsonify({"error": "Requisi√ß√£o deve ser JSON"}), 400

        data = request.get_json(force=True, silent=True)
        if not data:
            return jsonify({"error": "JSON inv√°lido ou vazio"}), 400

        question = data.get('question', '').strip()
        personality_id = data.get('personality_id')

        if not question:
            return jsonify({"error": "Pergunta n√£o fornecida"}), 400

        if not personality_id or personality_id not in ['dr_gasnelio', 'ga']:
            return jsonify({"error": "Personalidade inv√°lida"}), 400

        # Responder pergunta
        response = answer_question(question, personality_id)
        return jsonify(response)

    except Exception as e:
        logger.error(f"Erro na API de chat: {e}")
        return jsonify({"error": "Erro interno do servidor"}), 500

@app.route('/api/health', methods=['GET'])
def health_check():
    """Verifica√ß√£o de sa√∫de da API"""
    return jsonify({
        "status": "healthy",
        "model_loaded": qa_pipeline is not None,
        "pdf_loaded": len(pdf_text) > 0,
        "timestamp": datetime.now().isoformat()
    })

@app.route('/api/info', methods=['GET'])
def api_info():
    """Informa√ß√µes sobre a API"""
    return jsonify({
        "name": "Chatbot Tese Hansen√≠ase API",
        "version": "1.0.0",
        "description": "API para chatbot baseado na tese sobre roteiro de dispensa√ß√£o para hansen√≠ase",
        "personas": {
            "dr_gasnelio": "Professor s√©rio e t√©cnico",
            "ga": "Amigo descontra√≠do que explica de forma simples"
        },
        "model": "deepset/roberta-base-squad2",
        "pdf_source": "Roteiro de Dispensa√ß√£o para Hansen√≠ase"
    })

if __name__ == '__main__':
    # Inicializa√ß√£o
    logger.info("Iniciando aplica√ß√£o...")
    
    # Carrega o PDF
    if os.path.exists(PDF_PATH):
        pdf_text = extract_pdf_text(PDF_PATH)
    else:
        logger.warning(f"PDF n√£o encontrado: {PDF_PATH}")
        pdf_text = "PDF n√£o dispon√≠vel"
    
    # Carrega o modelo de IA
    load_ai_model()
    
    # Inicia o servidor
    port = int(os.environ.get('PORT', 5000))
    debug = os.environ.get('FLASK_ENV') == 'development'
    
    logger.info(f"Servidor iniciado na porta {port}")
    app.run(host='0.0.0.0', port=port, debug=debug) 