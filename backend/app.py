from flask import Flask, request, jsonify, render_template
from flask_cors import CORS
import os
from transformers import pipeline, AutoTokenizer, AutoModelForQuestionAnswering
import torch
import re
import logging
from datetime import datetime

app = Flask(__name__)
CORS(app)

# Configura√ß√£o de logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Vari√°veis globais
MD_PATH = '../PDFs/Roteiro de Dsispensa√ß√£o - Hansen√≠ase.md'
md_text = ""
qa_pipeline = None
tokenizer = None
model = None

def extract_md_text(md_path):
    """Extrai texto do arquivo Markdown"""
    global md_text
    try:
        with open(md_path, 'r', encoding='utf-8') as file:
            text = file.read()
        logger.info(f"Arquivo Markdown extra√≠do com sucesso. Total de caracteres: {len(text)}")
        return text
    except Exception as e:
        logger.error(f"Erro ao extrair arquivo Markdown: {e}")
        return ""

def load_ai_model():
    """Carrega o modelo de IA gratuito do Hugging Face"""
    global qa_pipeline, tokenizer, model
    try:
        # Usando modelo gratuito e leve
        model_name = "deepset/roberta-base-squad2"
        logger.info(f"Carregando modelo: {model_name}")
        
        qa_pipeline = pipeline(
            "question-answering",
            model=model_name,
            tokenizer=model_name,
            device=-1 if not torch.cuda.is_available() else 0
        )
        logger.info("Modelo carregado com sucesso")
    except Exception as e:
        logger.error(f"Erro ao carregar modelo: {e}")
        qa_pipeline = None

def find_relevant_context(question, full_text, max_length=4000):
    """Encontra o contexto mais relevante para a pergunta"""
    # Divide o texto em chunks menores
    chunks = []
    chunk_size = 2000
    overlap = 200
    
    for i in range(0, len(full_text), chunk_size - overlap):
        chunk = full_text[i:i + chunk_size]
        if chunk.strip():
            chunks.append(chunk)
    
    # Se temos poucos chunks, retorna o texto completo
    if len(chunks) <= 2:
        return full_text[:max_length]
    
    # Para perguntas simples, usa o primeiro chunk
    if len(question.split()) <= 5:
        return chunks[0][:max_length]
    
    # Busca por palavras-chave na pergunta
    question_words = set(re.findall(r'\w+', question.lower()))
    
    best_chunk = chunks[0]
    best_score = 0
    
    for chunk in chunks:
        chunk_words = set(re.findall(r'\w+', chunk.lower()))
        common_words = question_words.intersection(chunk_words)
        score = len(common_words) / len(question_words) if question_words else 0
        
        if score > best_score:
            best_score = score
            best_chunk = chunk
    
    return best_chunk[:max_length]

def answer_question(question, persona):
    """Responde √† pergunta usando o modelo de IA"""
    global qa_pipeline, md_text
    
    if not qa_pipeline or not md_text:
        return fallback_response(persona, "Modelo n√£o dispon√≠vel")
    
    try:
        # Encontra contexto relevante
        context = find_relevant_context(question, md_text)
        
        # Faz a pergunta ao modelo
        result = qa_pipeline(
            question=question,
            context=context,
            max_answer_len=200,
            handle_impossible_answer=True
        )
        
        answer = result['answer'].strip()
        confidence = result['score']
        
        logger.info(f"Pergunta: {question}")
        logger.info(f"Confian√ßa: {confidence}")
        logger.info(f"Resposta: {answer}")
        
        # Se a confian√ßa for baixa ou resposta vazia, usa fallback aprimorado
        if not answer or confidence < 0.3:
            logger.info("Usando fallback aprimorado - confian√ßa baixa")
            return enhanced_fallback_response(question, persona, context)
        
        return format_persona_answer(answer, persona, confidence)
        
    except Exception as e:
        logger.error(f"Erro ao processar pergunta: {e}")
        return fallback_response(persona, "Erro t√©cnico")

def format_persona_answer(answer, persona, confidence):
    """Formata a resposta de acordo com a personalidade, usando linguagem natural"""
    if persona == "dr_gasnelio":
        # Resposta t√©cnica, mas acolhedora e natural
        return {
            "answer": (
                f"Ol√°! Aqui √© o Dr. Gasnelio. Sobre sua d√∫vida:\n\n"
                f"{answer}\n\n"
                f"Se precisar de mais detalhes ou exemplos, posso explicar de outra forma.\n"
                f"*Baseado na tese sobre roteiro de dispensa√ß√£o para hansen√≠ase. Confian√ßa: {confidence:.1%}*"
            ),
            "persona": "dr_gasnelio",
            "confidence": confidence
        }
    elif persona == "ga":
        # Simplifica e deixa a resposta ainda mais pr√≥xima do cotidiano
        simple_answer = simplify_text(answer)
        return {
            "answer": (
                f"Oi! Eu sou o G√°. Olha s√≥, de um jeito bem simples:\n\n"
                f"{simple_answer}\n\n"
                f"Se quiser, posso dar um exemplo ou explicar de outro jeito. √â s√≥ pedir! üòä\n"
                f"*Essa explica√ß√£o veio direto da tese, mas falei do meu jeito pra facilitar.*"
            ),
            "persona": "ga",
            "confidence": confidence
        }
    else:
        return {
            "answer": answer,
            "persona": "default",
            "confidence": confidence
        }

def simplify_text(text):
    """Simplifica o texto para o G√°, deixando mais natural e pr√≥ximo do cotidiano"""
    replacements = {
        "dispensa√ß√£o": "entrega do rem√©dio na farm√°cia",
        "farmac√™utico": "farmac√™utico (quem trabalha na farm√°cia)",
        "medicamentos": "rem√©dios",
        "tratamento": "tratamento (o que a pessoa faz para melhorar)",
        "hansen√≠ase": "hansen√≠ase (doen√ßa de pele)",
        "protocolo": "guia de cuidados",
        "orienta√ß√£o": "explica√ß√£o",
        "paciente": "pessoa que est√° tratando",
        "ades√£o": "seguir direitinho o tratamento",
        "posologia": "como tomar o rem√©dio",
        "administra√ß√£o": "como usar o rem√©dio",
        "via de administra√ß√£o": "jeito de tomar o rem√©dio",
        "rea√ß√£o adversa": "efeito colateral (coisa ruim que pode acontecer)",
        "intera√ß√£o medicamentosa": "mistura de rem√©dios que pode dar problema",
        "supervisionada": "com algu√©m olhando junto",
        "autoadministrada": "a pr√≥pria pessoa toma sozinha",
        "prescri√ß√£o": "receita do m√©dico",
        "dose": "quantidade do rem√©dio",
        "contraindica√ß√£o": "quando n√£o pode usar",
        "indica√ß√£o": "quando √© recomendado usar"
    }
    simplified = text
    for technical, simple in replacements.items():
        simplified = simplified.replace(technical, simple)
    # Deixar frases mais curtas e naturais
    simplified = simplified.replace(". ", ".\n")
    return simplified

def enhanced_fallback_response(question, persona, context):
    """Fallback aprimorado que retorna trecho relevante do PDF"""
    logger.info("Executando fallback aprimorado")
    
    # Busca por palavras-chave na pergunta
    question_words = set(re.findall(r'\w+', question.lower()))
    
    # Divide o contexto em par√°grafos
    paragraphs = context.split('\n\n')
    
    best_paragraph = ""
    best_score = 0
    
    for paragraph in paragraphs:
        if len(paragraph.strip()) < 50:  # Ignora par√°grafos muito pequenos
            continue
            
        paragraph_words = set(re.findall(r'\w+', paragraph.lower()))
        common_words = question_words.intersection(paragraph_words)
        score = len(common_words) / len(question_words) if question_words else 0
        
        if score > best_score:
            best_score = score
            best_paragraph = paragraph
    
    # Se encontrou um par√°grafo relevante
    if best_paragraph and best_score > 0.1:
        logger.info(f"Par√°grafo relevante encontrado com score: {best_score}")
        
        if persona == "dr_gasnelio":
            return {
                "answer": f"Dr. Gasnelio responde:\n\nBaseado na minha tese sobre roteiro de dispensa√ß√£o para hansen√≠ase, encontrei esta informa√ß√£o t√©cnica relevante:\n\n\"{best_paragraph.strip()}\"\n\n*Esta √© uma extra√ß√£o direta do texto da tese. Para informa√ß√µes mais espec√≠ficas, recomendo consultar a se√ß√£o completa.*",
                "persona": "dr_gasnelio",
                "confidence": best_score
            }
        elif persona == "ga":
            # Simplifica o texto para o G√°
            simple_paragraph = simplify_text(best_paragraph.strip())
            return {
                "answer": f"G√° responde:\n\nOlha s√≥ o que encontrei na tese:\n\n\"{simple_paragraph}\"\n\n*T√° na tese, pode confiar! üòä*",
                "persona": "ga",
                "confidence": best_score
            }
        else:
            return {
                "answer": f"Encontrei esta informa√ß√£o na tese:\n\n\"{best_paragraph.strip()}\"",
                "persona": "default",
                "confidence": best_score
            }
    
    # Se n√£o encontrou nada relevante, usa fallback padr√£o
    logger.info("Nenhum par√°grafo relevante encontrado, usando fallback padr√£o")
    return fallback_response(persona, "Informa√ß√£o n√£o encontrada na tese")

def fallback_response(persona, reason=""):
    """Resposta de fallback quando n√£o encontra informa√ß√£o"""
    logger.info(f"Executando fallback padr√£o para persona: {persona}")
    
    if persona == "dr_gasnelio":
        return {
            "answer": f"Dr. Gasnelio responde:\n\nDesculpe, n√£o encontrei essa informa√ß√£o espec√≠fica na minha tese sobre roteiro de dispensa√ß√£o para hansen√≠ase. {reason}\n\nPosso ajud√°-lo com outras quest√µes relacionadas ao tema da pesquisa.",
            "persona": "dr_gasnelio",
            "confidence": 0.0
        }
    elif persona == "ga":
        return {
            "answer": f"G√° responde:\n\nIh, essa eu n√£o sei! üòÖ {reason}\n\nS√≥ posso explicar coisas que est√£o na tese sobre hansen√≠ase e dispensa√ß√£o de rem√©dios. Pergunta outra coisa sobre o tema?",
            "persona": "ga",
            "confidence": 0.0
        }
    else:
        return {
            "answer": f"Desculpe, n√£o encontrei essa informa√ß√£o na tese. {reason}",
            "persona": "default",
            "confidence": 0.0
        }

@app.route('/')
def index():
    """P√°gina principal"""
    return render_template('../templates/index.html')

@app.route('/tese')
def tese():
    return render_template('../templates/tese.html')

@app.route('/script.js')
def serve_script():
    """Serve o arquivo script.js na raiz do projeto"""
    return app.send_static_file('../static/script.js')

@app.route('/tese.js')
def serve_tese_script():
    """Serve o arquivo tese.js na raiz do projeto"""
    return app.send_static_file('../static/tese.js')

@app.route('/api/chat', methods=['POST'])
def chat_api():
    try:
        # Garante que o corpo √© JSON v√°lido
        if not request.is_json:
            return jsonify({"error": "Requisi√ß√£o deve ser JSON"}), 400

        data = request.get_json(force=True, silent=True)
        if not data:
            return jsonify({"error": "JSON inv√°lido ou vazio"}), 400

        question = data.get('question', '').strip()
        personality_id = data.get('personality_id')

        if not question:
            return jsonify({"error": "Pergunta n√£o fornecida"}), 400

        if not personality_id or personality_id not in ['dr_gasnelio', 'ga']:
            return jsonify({"error": "Personalidade inv√°lida"}), 400

        # Responder pergunta
        response = answer_question(question, personality_id)
        return jsonify(response)

    except Exception as e:
        logger.error(f"Erro na API de chat: {e}")
        return jsonify({"error": "Erro interno do servidor"}), 500

@app.route('/api/health', methods=['GET'])
def health_check():
    """Verifica√ß√£o de sa√∫de da API"""
    return jsonify({
        "status": "healthy",
        "model_loaded": qa_pipeline is not None,
        "pdf_loaded": len(md_text) > 0,
        "timestamp": datetime.now().isoformat()
    })

@app.route('/api/info', methods=['GET'])
def api_info():
    """Informa√ß√µes sobre a API"""
    return jsonify({
        "name": "Chatbot Tese Hansen√≠ase API",
        "version": "1.0.0",
        "description": "API para chatbot baseado na tese sobre roteiro de dispensa√ß√£o para hansen√≠ase",
        "personas": {
            "dr_gasnelio": "Professor s√©rio e t√©cnico",
            "ga": "Amigo descontra√≠do que explica de forma simples"
        },
        "model": "deepset/roberta-base-squad2",
        "pdf_source": "Roteiro de Dispensa√ß√£o para Hansen√≠ase"
    })

if __name__ == '__main__':
    # Inicializa√ß√£o
    logger.info("Iniciando aplica√ß√£o...")
    
    # Carrega o PDF
    if os.path.exists(MD_PATH):
        md_text = extract_md_text(MD_PATH)
    else:
        logger.warning(f"Arquivo Markdown n√£o encontrado: {MD_PATH}")
        md_text = "Arquivo Markdown n√£o dispon√≠vel"
    
    # Carrega o modelo de IA
    load_ai_model()
    
    # Inicia o servidor
    port = int(os.environ.get('PORT', 5000))
    debug = os.environ.get('FLASK_ENV') == 'development'
    
    logger.info(f"Servidor iniciado na porta {port}")
    app.run(host='0.0.0.0', port=port, debug=debug) 