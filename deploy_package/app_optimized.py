from flask import Flask, request, jsonify, render_template
from flask_cors import CORS
import os
from transformers.pipelines import pipeline
from transformers import AutoTokenizer, AutoModelForQuestionAnswering, AutoModelForCausalLM, AutoModelForSequenceClassification
import torch
import re
import logging
from datetime import datetime
import random
import json
import requests

app = Flask(__name__)
CORS(app)

# Configura√ß√£o de logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Vari√°veis globais
MD_PATH = 'PDFs/Roteiro de Dsispensa√ß√£o - Hansen√≠ase.md'
md_text = ""
qa_pipeline = None
text_generation_pipeline = None
sentiment_pipeline = None
tokenizer = None
model = None

# Tr√™s chaves e modelos
OPENROUTER_API_KEY_LLAMA = os.environ.get("OPENROUTER_API_KEY_LLAMA", "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")
OPENROUTER_API_KEY_QWEN = os.environ.get("OPENROUTER_API_KEY_QWEN", "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")
OPENROUTER_API_KEY_GEMINI = os.environ.get("OPENROUTER_API_KEY_GEMINI", "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")
LLAMA3_MODEL = "meta-llama/llama-3.3-70b-instruct:free"
QWEN_MODEL = "qwen/qwen3-14b:free"
GEMINI_MODEL = "google/gemini-2.0-flash-exp:free"

# Templates de linguagem natural para cada persona
NATURAL_TEMPLATES = {
    "dr_gasnelio": {
        "greeting": [
            "Sauda√ß√µes! Sou o Dr. Gasnelio. Minha pesquisa foca no roteiro de dispensa√ß√£o para a pr√°tica da farm√°cia cl√≠nica. Como posso auxili√°-lo hoje?",
            "Ol√°! Aqui √© o Dr. Gasnelio. Tenho dedicado minha carreira ao estudo da dispensa√ß√£o farmac√™utica. Em que posso ajud√°-lo?",
            "Bem-vindo! Sou o Dr. Gasnelio, especialista em farm√°cia cl√≠nica. Como posso contribuir com sua consulta hoje?"
        ],
        "thinking": [
            "Deixe-me analisar essa quest√£o com base na minha pesquisa...",
            "Interessante pergunta. Vou consultar os dados da tese...",
            "Essa √© uma quest√£o importante. Permita-me buscar nas fontes..."
        ],
        "confidence_high": [
            "Baseado na minha pesquisa, posso afirmar com confian√ßa que:",
            "Os dados da tese s√£o bastante claros sobre isso:",
            "Minha an√°lise da literatura confirma que:"
        ],
        "confidence_medium": [
            "Com base no que encontrei na tese, posso sugerir que:",
            "Os dados dispon√≠veis indicam que:",
            "Baseado na pesquisa, parece que:"
        ],
        "confidence_low": [
            "Essa quest√£o √© interessante, mas n√£o encontrei dados espec√≠ficos na tese. Sugiro consultar:",
            "N√£o tenho informa√ß√µes detalhadas sobre isso na pesquisa, mas posso orientar para:",
            "Essa √°rea n√£o foi coberta especificamente na tese, mas posso sugerir:"
        ]
    },
    "ga": {
        "greeting": [
            "Opa, tudo certo? Aqui √© o G√°! T√¥ aqui pra gente desenrolar qualquer d√∫vida sobre o uso correto de medicamentos e o roteiro de dispensa√ß√£o. Manda a ver!",
            "E a√≠, beleza? Sou o G√°! T√¥ aqui pra te ajudar com qualquer parada sobre rem√©dios e farm√°cia. Fala a√≠!",
            "Oi! Aqui √© o G√°! T√¥ aqui pra gente conversar sobre medicamentos e como usar direitinho. Qual √© a boa?"
        ],
        "thinking": [
            "Deixa eu dar uma olhada na tese aqui...",
            "Hmm, deixa eu ver o que tem sobre isso...",
            "Vou procurar essa informa√ß√£o pra voc√™..."
        ],
        "confidence_high": [
            "Olha s√≥, encontrei isso na tese:",
            "T√° aqui, direto da pesquisa:",
            "D√° uma olhada nisso que achei:"
        ],
        "confidence_medium": [
            "Olha, pelo que vi na tese:",
            "Acho que √© mais ou menos assim:",
            "Pelo que entendi da pesquisa:"
        ],
        "confidence_low": [
            "Ih, essa eu n√£o sei certinho, mas posso te ajudar a procurar!",
            "N√£o achei essa informa√ß√£o espec√≠fica, mas posso te orientar!",
            "Essa parte n√£o t√° muito clara na tese, mas vamos ver o que tem!"
        ]
    }
}

def extract_md_text(md_path):
    """Extrai texto do arquivo Markdown"""
    global md_text
    try:
        with open(md_path, "r", encoding="utf-8") as file:
            text = file.read()
        logger.info(f"Arquivo Markdown extra√≠do com sucesso. Total de caracteres: {len(text)}")
        return text
    except Exception as e:
        logger.error(f"Erro ao extrair arquivo Markdown: {e}")
        return ""

def load_ai_models():
    """Carrega m√∫ltiplos modelos de IA gratuitos do Hugging Face"""
    global qa_pipeline, text_generation_pipeline, sentiment_pipeline, tokenizer, model
    try:
        # Modelo principal para QA (mais robusto)
        model_name = "deepset/roberta-base-squad2"
        logger.info(f"Carregando modelo QA: {model_name}")
        
        qa_pipeline = pipeline(
            "question-answering",
            model=model_name,
            tokenizer=model_name,
            device=-1 if not torch.cuda.is_available() else 0
        )
        
        # Modelo para gera√ß√£o de texto (mais natural)
        generation_model = "microsoft/DialoGPT-medium"
        logger.info(f"Carregando modelo de gera√ß√£o: {generation_model}")
        
        text_generation_pipeline = pipeline(
            "text-generation",
            model=generation_model,
            tokenizer=generation_model,
            device=-1 if not torch.cuda.is_available() else 0,
            max_length=100,
            do_sample=True,
            temperature=0.7
        )
        
        # Modelo para an√°lise de sentimento/contexto (comentado temporariamente)
        # sentiment_model = "cardiffnlp/twitter-roberta-base-sentiment"
        # logger.info(f"Carregando modelo de sentimento: {sentiment_model}")
        
        # sentiment_pipeline = pipeline(
        #     "sentiment-analysis",
        #     model=sentiment_model,
        #     device=-1 if not torch.cuda.is_available() else 0
        # )
        sentiment_pipeline = None
        
        logger.info("Todos os modelos carregados com sucesso")
    except Exception as e:
        logger.error(f"Erro ao carregar modelos: {e}")
        # Fallback para modelo √∫nico se houver erro
        try:
            qa_pipeline = pipeline(
                "question-answering",
                model="deepset/roberta-base-squad2",
                device=-1 if not torch.cuda.is_available() else 0
            )
        except Exception as e2:
            logger.error(f"Erro no fallback: {e2}")
            qa_pipeline = None

def get_natural_phrase(persona, category, confidence_level="medium"):
    """Retorna uma frase natural baseada na persona e contexto"""
    templates = NATURAL_TEMPLATES.get(persona, {})
    category_templates = templates.get(category, [])
    
    if category == "confidence":
        confidence_key = f"confidence_{confidence_level}"
        category_templates = templates.get(confidence_key, [])
    
    if category_templates:
        return random.choice(category_templates)
    return ""

def find_relevant_context_enhanced(question, full_text, max_length=800):
    """Encontra contexto mais relevante usando m√∫ltiplas estrat√©gias"""
    # Divide o texto em chunks menores com overlap
    chunks = []
    chunk_size = 1000  # Aumentado para pegar mais contexto
    overlap = 300      # Aumentado o overlap
    
    for i in range(0, len(full_text), chunk_size - overlap):
        chunk = full_text[i:i + chunk_size]
        if chunk.strip():
            chunks.append(chunk)
    
    if len(chunks) <= 2:
        return full_text[:max_length]
    
    # Busca por palavras-chave na pergunta
    question_words = set(re.findall(r'\w+', question.lower()))
    
    best_chunks = []
    chunk_scores = []
    
    for chunk in chunks:
        chunk_words = set(re.findall(r'\w+', chunk.lower()))
        common_words = question_words.intersection(chunk_words)
        score = len(common_words) / len(question_words) if question_words else 0
        
        # B√¥nus para chunks que cont√™m termos m√©dicos espec√≠ficos
        medical_terms = ['medicamento', 'dose', 'tratamento', 'rea√ß√£o', 'efeito', 'hansen√≠ase', 'clofazimina', 'rifampicina', 'dapsona', 'acompanhamento', 'dispensa√ß√£o', 'paciente']
        for term in medical_terms:
            if term in chunk.lower():
                score += 0.1
        
        chunk_scores.append((chunk, score))
    
    # Ordena por score e pega os melhores
    chunk_scores.sort(key=lambda x: x[1], reverse=True)
    
    # Combina os 2 melhores chunks
    combined_context = ""
    for chunk, score in chunk_scores[:2]:
        if score > 0.05:  # Threshold m√≠nimo
            combined_context += chunk + "\n\n"
    
    return combined_context[:max_length] if combined_context else chunks[0][:max_length]

def enhance_response_with_generation(base_answer, question, persona):
    """Melhora a resposta usando gera√ß√£o de texto natural"""
    if not text_generation_pipeline or not base_answer:
        return base_answer
    
    try:
        # Cria um prompt contextual
        if persona == "dr_gasnelio":
            prompt = f"Como um farmac√™utico especialista, responda de forma t√©cnica mas acess√≠vel: {question} Resposta base: {base_answer}"
        else:
            prompt = f"Como um farmac√™utico amig√°vel, explique de forma simples: {question} Resposta base: {base_answer}"
        
        # Gera texto complementar
        generated = text_generation_pipeline(
            prompt,
            max_length=len(prompt.split()) + 20,
            do_sample=True,
            temperature=0.6
        )
        
        if generated and len(generated) > 0:
            enhanced_text = generated[0]['generated_text']
            # Extrai apenas a parte gerada (remove o prompt)
            if len(enhanced_text) > len(prompt):
                new_content = enhanced_text[len(prompt):].strip()
                if new_content:
                    return f"{base_answer}\n\n{new_content}"
        
        return base_answer
    except Exception as e:
        logger.error(f"Erro na gera√ß√£o de texto: {e}")
        return base_answer

def answer_question_enhanced(question, persona, conversation_history=None):
    """Resposta aprimorada usando m√∫ltiplos modelos"""
    global qa_pipeline, md_text
    
    if not qa_pipeline or not md_text:
        return enhanced_fallback_response(question, persona, "")
    
    try:
        # Encontra contexto relevante
        context = find_relevant_context_enhanced(question, md_text)
        
        # Faz a pergunta ao modelo QA
        result = qa_pipeline(
            question=question,
            context=context,
            max_answer_len=300,
            handle_impossible_answer=True
        )
        
        # Acessa os resultados de forma segura
        if isinstance(result, dict):
            answer = result.get('answer', '').strip()
            confidence = result.get('score', 0.0)
        else:
            answer = ''
            confidence = 0.0
        
        logger.info(f"Pergunta: {question}")
        logger.info(f"Confian√ßa QA: {confidence}")
        logger.info(f"Resposta base: {answer}")
        
        # Determina o n√≠vel de confian√ßa
        if confidence > 0.6:
            confidence_level = "high"
        elif confidence > 0.3:
            confidence_level = "medium"
        else:
            confidence_level = "low"
        
        # Se a confian√ßa for baixa ou resposta vazia, usa fallback aprimorado
        if not answer or confidence < 0.2:
            logger.info("Usando fallback aprimorado - confian√ßa baixa")
            return enhanced_fallback_response(question, persona, context)
        
        # Melhora a resposta com gera√ß√£o de texto
        enhanced_answer = enhance_response_with_generation(answer, question, persona)
        
        # Formata com linguagem natural
        return format_persona_answer_enhanced(enhanced_answer, persona, confidence_level)
        
    except Exception as e:
        logger.error(f"Erro ao processar pergunta: {e}")
        return enhanced_fallback_response(question, persona, "")

def format_persona_answer_enhanced(answer, persona, confidence_level):
    """Formata a resposta com linguagem natural aprimorada"""
    
    # Pega frase de introdu√ß√£o baseada na confian√ßa
    intro_phrase = get_natural_phrase(persona, "confidence", confidence_level)
    
    if persona == "dr_gasnelio":
        return {
            "answer": (
                f"Dr. Gasnelio responde:\n\n"
                f"{intro_phrase}\n\n"
                f"{answer}\n\n"
                f"*Baseado na minha tese sobre roteiro de dispensa√ß√£o para hansen√≠ase. "
                f"N√≠vel de confian√ßa: {'Alto' if confidence_level == 'high' else 'M√©dio' if confidence_level == 'medium' else 'Baixo'}.*"
            ),
            "persona": "dr_gasnelio",
            "confidence": confidence_level
        }
    elif persona == "ga":
        # Transforma completamente a resposta para o G√° - mais descontra√≠da e explicativa
        simple_answer = transform_for_ga(answer)
        return {
            "answer": (
                f"G√° responde:\n\n"
                f"{intro_phrase}\n\n"
                f"{simple_answer}\n\n"
                f"*T√° na tese, pode confiar! üòä*"
            ),
            "persona": "ga",
            "confidence": confidence_level
        }
    else:
        return {
            "answer": answer,
            "persona": "default",
            "confidence": confidence_level
        }

def transform_for_ga(text):
    """Transforma o texto t√©cnico em linguagem descontra√≠da e explicativa para o G√°"""
    # Remove aspas e formata√ß√µes t√©cnicas
    text = text.replace('"', '').replace('"', '').replace('"', '')
    
    # Simplifica termos t√©cnicos
    replacements = {
        "dispensa√ß√£o": "entrega do rem√©dio na farm√°cia",
        "medicamentos": "rem√©dios",
        "posologia": "como tomar o rem√©dio",
        "administra√ß√£o": "como usar o rem√©dio",
        "rea√ß√£o adversa": "efeito colateral",
        "intera√ß√£o medicamentosa": "mistura de rem√©dios que pode dar problema",
        "protocolo": "guia de cuidados",
        "orienta√ß√£o": "explica√ß√£o",
        "ades√£o": "seguir direitinho o tratamento",
        "tratamento": "tratamento",
        "hansen√≠ase": "hansen√≠ase",
        "paciente": "pessoa que est√° tratando",
        "supervisionada": "com algu√©m olhando junto",
        "autoadministrada": "a pr√≥pria pessoa toma sozinha",
        "prescri√ß√£o": "receita do m√©dico",
        "dose": "quantidade do rem√©dio",
        "contraindica√ß√£o": "quando n√£o pode usar",
        "indica√ß√£o": "quando √© recomendado usar",
        "poliquimioterapia": "mistura de rem√©dios",
        "rifampicina": "rifampicina",
        "clofazimina": "clofazimina",
        "dapsona": "dapsona",
        "mg": "miligramas",
        "dose mensal": "rem√©dio que toma uma vez por m√™s",
        "dose di√°ria": "rem√©dio que toma todo dia",
        "supervisionada": "com algu√©m olhando",
        "autoadministrada": "a pessoa toma sozinha"
    }
    
    # Aplica as substitui√ß√µes
    for technical, simple in replacements.items():
        text = text.replace(technical, simple)
    
    # Adiciona express√µes descontra√≠das
    casual_expressions = [
        "Olha s√≥, ",
        "Ent√£o, ",
        "Tipo assim, ",
        "Sabe como √©, ",
        "√â o seguinte, ",
        "Fica ligado, ",
        "Cara, ",
        "Mano, ",
        "Beleza, ",
        "Tranquilo, "
    ]
    
    # Quebra o texto em frases e adiciona express√µes casuais
    sentences = text.split('. ')
    if sentences and len(sentences) > 0:
        first_sentence = sentences[0]
        if not any(expr in first_sentence for expr in casual_expressions):
            sentences[0] = random.choice(casual_expressions) + first_sentence.lower()
    
    # Reconstr√≥i o texto
    transformed_text = '. '.join(sentences)
    
    # Adiciona emojis e express√µes informais
    emoji_replacements = {
        "importante": "importante ‚ö†Ô∏è",
        "cuidado": "cuidado ‚ö†Ô∏è",
        "aten√ß√£o": "aten√ß√£o üëÄ",
        "lembre": "lembre üí°",
        "consulte": "consulte üë®‚Äç‚öïÔ∏è",
        "m√©dico": "m√©dico üë®‚Äç‚öïÔ∏è",
        "farmac√™utico": "farmac√™utico üíä",
        "rem√©dio": "rem√©dio üíä",
        "tratamento": "tratamento üè•"
    }
    
    for word, replacement in emoji_replacements.items():
        transformed_text = transformed_text.replace(word, replacement)
    
    return transformed_text

def enhanced_fallback_response(question, persona, context):
    """Fallback aprimorado que retorna trecho relevante do PDF"""
    logger.info("Executando fallback aprimorado")
    
    # Busca por palavras-chave na pergunta
    question_words = set(re.findall(r'\w+', question.lower()))
    
    # Divide o contexto em par√°grafos
    paragraphs = context.split('\n\n') if context else md_text.split('\n\n')
    
    best_paragraph = None
    best_score = 0.0
    
    for paragraph in paragraphs:
        if len(paragraph.strip()) < 50:  # Ignora par√°grafos muito pequenos
            continue
            
        paragraph_words = set(re.findall(r'\w+', paragraph.lower()))
        common_words = question_words.intersection(paragraph_words)
        score = len(common_words) / len(question_words) if question_words else 0
        
        if score > best_score:
            best_score = score
            best_paragraph = paragraph
    
    # Se encontrou um par√°grafo relevante
    if best_paragraph and best_score > 0.1:
        logger.info(f"Par√°grafo relevante encontrado com score: {best_score}")
        
        if persona == "dr_gasnelio":
            # Para o Dr. Gasnelio, retorna o par√°grafo completo sem cortes
            complete_paragraph = best_paragraph.strip()
            
            # Se o par√°grafo parece estar cortado, tenta encontrar o contexto completo
            if complete_paragraph.endswith('...') or len(complete_paragraph) < 100:
                # Busca por par√°grafos relacionados
                related_paragraphs = []
                for para in paragraphs:
                    if any(word in para.lower() for word in question_words):
                        related_paragraphs.append(para.strip())
                
                if related_paragraphs:
                    complete_paragraph = '\n\n'.join(related_paragraphs[:2])  # Pega at√© 2 par√°grafos relacionados
            
            return {
                "answer": (
                    f"Dr. Gasnelio responde:\n\n"
                    f"Baseado na minha tese sobre roteiro de dispensa√ß√£o para hansen√≠ase, encontrei esta informa√ß√£o t√©cnica relevante:\n\n"
                    f"\"{complete_paragraph}\"\n\n"
                    f"*Esta √© uma extra√ß√£o direta do texto da tese. Para informa√ß√µes mais espec√≠ficas, recomendo consultar a se√ß√£o completa.*"
                ),
                "persona": "dr_gasnelio",
                "confidence": "low"
            }
        elif persona == "ga":
            # Transforma completamente para o G√° - n√£o copia texto direto
            ga_explanation = explain_like_ga(best_paragraph.strip(), question)
            return {
                "answer": (
                    f"G√° responde:\n\n"
                    f"Olha s√≥, encontrei algumas informa√ß√µes sobre isso na tese! üòä\n\n"
                    f"{ga_explanation}\n\n"
                    f"*T√° na tese, pode confiar! Mas se tiver d√∫vida, √© s√≥ perguntar de novo! üòâ*"
                ),
                "persona": "ga",
                "confidence": "low"
            }
        else:
            return {
                "answer": (
                    f"Encontrei esta informa√ß√£o na tese:\n\n"
                    f"\"{best_paragraph.strip()}\""
                ),
                "persona": "default",
                "confidence": "low"
            }
    
    # Se n√£o encontrou nada relevante, usa fallback padr√£o
    logger.info("Nenhum par√°grafo relevante encontrado, usando fallback padr√£o")
    return fallback_response(persona, "Informa√ß√£o n√£o encontrada na tese")

def explain_like_ga(technical_text, question):
    """Explica o texto t√©cnico de forma descontra√≠da como o G√° faria"""
    
    # Identifica o tipo de pergunta para dar uma resposta mais contextual
    question_lower = question.lower()
    
    if "rea√ß√£o" in question_lower or "efeito" in question_lower or "colateral" in question_lower:
        return (
            "Cara, sobre efeitos colaterais, √© sempre bom ficar ligado! ‚ö†Ô∏è "
            "Os rem√©dios podem dar algumas rea√ß√µes, sabe? Tipo, pode dar dor de barriga, "
            "enjoo, ou at√© mudar a cor da pele. Mas n√£o se preocupa, isso √© normal! "
            "O importante √© sempre conversar com o m√©dico se sentir algo estranho. "
            "E lembra: cada pessoa reage diferente, ent√£o n√£o se compara com os outros! üòä"
        )
    
    elif "dose" in question_lower or "como tomar" in question_lower or "posologia" in question_lower:
        return (
            "Beleza, sobre como tomar os rem√©dios! üíä "
            "Tem alguns que voc√™ toma todo dia, outros que toma uma vez por m√™s com algu√©m olhando. "
            "√â tipo assim: alguns s√£o pra tomar com comida, outros n√£o podem tomar com suco de laranja. "
            "O importante √© seguir direitinho o que o m√©dico passou! "
            "Se esquecer de tomar, n√£o se desespera, s√≥ toma quando lembrar. "
            "Mas se estiver muito perto da pr√≥xima dose, pula a que esqueceu! üòâ"
        )
    
    elif "acompanhamento" in question_lower or "seguimento" in question_lower:
        return (
            "Ent√£o, sobre o acompanhamento! üë®‚Äç‚öïÔ∏è "
            "√â super importante ir nas consultas direitinho, sabe? "
            "O m√©dico vai ficar de olho pra ver se est√° tudo funcionando bem. "
            "Tem que fazer alguns exames tamb√©m, pra garantir que est√° tudo certo. "
            "E se sentir qualquer coisa estranha, corre falar com eles! "
            "N√£o fica com vergonha de perguntar, eles est√£o l√° pra isso! üòä"
        )
    
    elif "hansen√≠ase" in question_lower or "tratamento" in question_lower:
        return (
            "Cara, sobre o tratamento da hansen√≠ase! üè• "
            "√â um tratamento que demora um pouco, mas funciona super bem! "
            "Voc√™ vai tomar alguns rem√©dios juntos, tipo uma mistura. "
            "Alguns s√£o pra tomar todo dia, outros uma vez por m√™s. "
            "O importante √© n√£o parar no meio, mesmo que melhore! "
            "Se parar, pode voltar pior. Ent√£o firmeza, vamo at√© o final! üí™"
        )
    
    else:
        # Resposta gen√©rica mas descontra√≠da
        return (
            "Olha s√≥, encontrei algumas informa√ß√µes legais sobre isso! üí° "
            "√â sempre bom ficar ligado nos detalhes, sabe? "
            "Cada coisa tem seu jeito certo de fazer. "
            "Se tiver d√∫vida sobre algo espec√≠fico, √© s√≥ perguntar! "
            "T√¥ aqui pra ajudar! üòä"
        )

def fallback_response(persona, reason=""):
    """Resposta de fallback quando n√£o encontra informa√ß√£o"""
    logger.info(f"Executando fallback padr√£o para persona: {persona}")
    
    if persona == "dr_gasnelio":
        return {
            "answer": (
                f"Dr. Gasnelio responde:\n\n"
                f"N√£o encontrei uma resposta exata na tese para sua pergunta. Recomendo consultar as se√ß√µes de Seguran√ßa, Rea√ß√µes Adversas ou Intera√ß√µes para mais detalhes.\n\n"
                f"Se quiser, posso tentar explicar de outra forma ou buscar em outra parte do texto. Fique √† vontade para perguntar novamente! {reason}"
            ),
            "persona": "dr_gasnelio",
            "confidence": "low"
        }
    elif persona == "ga":
        return {
            "answer": (
                f"G√° responde:\n\n"
                f"Ih, n√£o achei uma resposta certinha na tese, mas posso te ajudar a procurar! D√° uma olhada nas partes de efeitos colaterais ou seguran√ßa, ou me pergunte de outro jeito que eu tento de novo! {reason}"
            ),
            "persona": "ga",
            "confidence": "low"
        }
    else:
        return {
            "answer": (
                f"Desculpe, n√£o encontrei essa informa√ß√£o na tese. {reason}"
            ),
            "persona": "default",
            "confidence": "low"
        }

def call_openrouter_model(question, context, persona, model, api_key):
    url = "https://openrouter.ai/api/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json",
        "HTTP-Referer": "http://localhost:5000",
        "X-Title": "Chatbot Tese Hanseniase"
    }
    if persona == "dr_gasnelio":
        system_prompt = (
            "Voc√™ √© o Dr. Gasnelio, farmac√™utico pesquisador, responde de forma t√©cnica, formal e baseada em evid√™ncias. Use o contexto abaixo para responder de forma precisa e objetiva."
        )
    else:
        system_prompt = (
            "Voc√™ √© a G√°, uma assistente amig√°vel e did√°tica. Explique de forma simples, acess√≠vel e acolhedora, usando o contexto abaixo."
        )
    payload = {
        "model": model,
        "messages": [
            {"role": "system", "content": f"{system_prompt}\n\nContexto:\n{context}"},
            {"role": "user", "content": question}
        ]
    }
    try:
        response = requests.post(url, headers=headers, json=payload, timeout=60)
        if response.status_code != 200:
            raise Exception(f"Erro OpenRouter: {response.status_code} - {response.text}")
        data = response.json()
        return data['choices'][0]['message']['content']
    except Exception as e:
        print(f"Erro ao chamar OpenRouter ({model}): {e}\nResposta: {getattr(e, 'response', None)}")
        return None

# Fun√ß√£o principal com fallback (Llama -> Qwen -> Gemini)
def call_chatbot_with_fallback(question, context, persona):
    # 1. Llama
    resposta = call_openrouter_model(question, context, persona, LLAMA3_MODEL, OPENROUTER_API_KEY_LLAMA)
    if resposta:
        return resposta
    # 2. Qwen
    resposta = call_openrouter_model(question, context, persona, QWEN_MODEL, OPENROUTER_API_KEY_QWEN)
    if resposta:
        return resposta
    # 3. Gemini
    resposta = call_openrouter_model(question, context, persona, GEMINI_MODEL, OPENROUTER_API_KEY_GEMINI)
    if resposta:
        return resposta
    return "[Erro ao consultar os modelos OpenRouter. Por favor, tente novamente mais tarde.]"

@app.route('/')
def index():
    """P√°gina principal"""
    return render_template('index.html')

@app.route('/tese')
def tese():
    return render_template('tese.html')

@app.route('/script.js')
def serve_script():
    """Serve o arquivo script.js na raiz do projeto"""
    return app.send_static_file('script.js')

@app.route('/test')
def test_page():
    """P√°gina de teste"""
    return render_template('test_js.html')

@app.route('/api/chat', methods=['POST'])
def chat_api():
    try:
        if not request.is_json:
            return jsonify({"error": "Requisi√ß√£o deve ser JSON"}), 400
        data = request.get_json(force=True, silent=True)
        if not data:
            return jsonify({"error": "JSON inv√°lido ou vazio"}), 400
        question = data.get('question', '').strip()
        personality_id = data.get('personality_id', 'dr_gasnelio')
        if not question:
            return jsonify({"error": "Pergunta n√£o fornecida"}), 400
        if personality_id not in ['dr_gasnelio', 'ga']:
            return jsonify({"error": "Personalidade inv√°lida"}), 400
        # Extrai contexto relevante do Markdown
        context = find_relevant_context_enhanced(question, md_text, max_length=800)
        # Chama o modelo Llama-3 via OpenRouter
        resposta = call_chatbot_with_fallback(question, context, personality_id)
        return jsonify({"answer": resposta})
    except Exception as e:
        logger.error(f"Erro na API de chat: {e}")
        return jsonify({"error": "Erro interno do servidor"}), 500

@app.route('/api/health', methods=['GET'])
def health_check():
    """Verifica√ß√£o de sa√∫de da API"""
    return jsonify({
        "status": "healthy",
        "qa_model_loaded": qa_pipeline is not None,
        "generation_model_loaded": text_generation_pipeline is not None,
        "sentiment_model_loaded": sentiment_pipeline is not None,
        "md_loaded": len(md_text) > 0,
        "timestamp": datetime.now().isoformat()
    })

@app.route('/api/info', methods=['GET'])
def api_info():
    """Informa√ß√µes sobre a API"""
    return jsonify({
        "name": "Chatbot Tese Hansen√≠ase API - Vers√£o H√≠brida",
        "version": "3.0.0",
        "description": "API h√≠brida otimizada para chatbot baseado na tese sobre roteiro de dispensa√ß√£o para hansen√≠ase, combinando m√∫ltiplos modelos de IA gratuitos",
        "personas": {
            "dr_gasnelio": "Professor s√©rio e t√©cnico",
            "ga": "Amigo descontra√≠do que explica de forma simples"
        },
        "models": {
            "qa_model": "deepset/roberta-base-squad2",
            "generation_model": "microsoft/DialoGPT-medium",
            "sentiment_model": "cardiffnlp/twitter-roberta-base-sentiment"
        },
        "source": "Roteiro de Dispensa√ß√£o para Hansen√≠ase (Markdown)",
        "features": [
            "Sistema h√≠brido com m√∫ltiplos modelos de IA",
            "Linguagem natural contextual para ambas as personas",
            "Gera√ß√£o de texto complementar",
            "An√°lise de sentimento para contexto",
            "Fallback aprimorado com trechos relevantes",
            "Simplifica√ß√£o autom√°tica para o G√°",
            "Contexto inteligente para perguntas"
        ]
    })

if __name__ == '__main__':
    # Inicializa√ß√£o
    logger.info("Iniciando aplica√ß√£o h√≠brida otimizada...")
    
    # Carrega o arquivo Markdown
    if os.path.exists(MD_PATH):
        md_text = extract_md_text(MD_PATH)
    else:
        logger.warning(f"Arquivo Markdown n√£o encontrado: {MD_PATH}")
        md_text = "Arquivo Markdown n√£o dispon√≠vel"
    
    # Carrega os modelos de IA
    load_ai_models()
    
    # Inicia o servidor
    port = int(os.environ.get('PORT', 5000))
    debug = os.environ.get('FLASK_ENV') == 'development'
    
    logger.info(f"Servidor h√≠brido otimizado iniciado na porta {port}")
    app.run(host='0.0.0.0', port=port, debug=debug) 